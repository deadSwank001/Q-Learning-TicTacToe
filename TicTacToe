"""Creating a Tic Tac Toe bot that uses reinforcement learning (RL)
can be a fun and insightful project. Given that Tic Tac Toe is a 
relatively simple game, you can implement a bot with basic RL techniques 
without requiring a complex neural network architecture. Hereâ€™s a straightforward approach:"""

#1. Game Representation
#First, represent the Tic Tac Toe board and the game state. You can represent the board as a 3x3 matrix (or a 1D list of length 9):

import numpy as np

class TicTacToe:
    def __init__(self):
        self.board = np.zeros((3, 3), dtype=int)
        self.player_turn = 1  # player 1 starts

    def reset(self):
        self.board.fill(0)
        self.player_turn = 1

    def make_move(self, x, y):
        if self.board[x, y] == 0:
            self.board[x, y] = self.player_turn
            reward = self.check_winner()
            self.player_turn = 3 - self.player_turn  # switch player
            return reward
        return 0

    def check_winner(self):
        # Check rows, columns, and diagonals for a winner
        for i in range(3):
            if abs(sum(self.board[i, :])) == 3:  # check rows
                return 1 if self.board[i, 0] == 1 else -1
            if abs(sum(self.board[:, i])) == 3:  # check columns
                return 1 if self.board[0, i] == 1 else -1
        if abs(sum(self.board.diagonal())) == 3:
            return 1 if self.board[0, 0] == 1 else -1
        if abs(sum(np.fliplr(self.board).diagonal())) == 3:
            return 1 if self.board[0, 2] == 1 else -1
        if np.all(self.board != 0):  # tie condition
            return 0
        return None  # game continues

#2. Q-Learning Approach
#Since the state space is relatively small (only 3x3 grids), you can use Q-learning without a neural network. Instead of a function approximator, you will maintain a Q-table.

#Q-Table Initialization
#Create a Q-table where each state-action pair has an associated Q-value.

import random

class TicTacToeAgent:
    def __init__(self, epsilon=0.1, alpha=0.5, gamma=0.9):
        self.q_table = {}
        self.epsilon = epsilon  # exploration rate
        self.alpha = alpha      # learning rate
        self.gamma = gamma      # discount factor

    def get_q_value(self, state, action):
        return self.q_table.get((tuple(state.flatten()), action), 0.0)

    def update_q_value(self, state, action, reward, next_state):
        max_future_q = max(self.get_q_value(next_state, a) for a in self.get_available_actions(next_state))
        current_q = self.get_q_value(state, action)
        new_q = (1 - self.alpha) * current_q + self.alpha * (reward + self.gamma * max_future_q)
        self.q_table[(tuple(state.flatten()), action)] = new_q

    def get_available_actions(self, state):
        return [(i, j) for i in range(3) for j in range(3) if state[i, j] == 0]

    def choose_action(self, state):
        if random.random() < self.epsilon:  # exploration
            return random.choice(self.get_available_actions(state))
        else:  # exploitation
            q_values = [self.get_q_value(state, action) for action in self.get_available_actions(state)]
            max_q_value = max(q_values)
            max_actions = [action for action in self.get_available_actions(state) if self.get_q_value(state, action) == max_q_value]
            return random.choice(max_actions)

#3. Training the Agent
#You can train the agent by simulating games against itself (or a predefined opponent) to learn from the outcomes.

def train(agent, episodes=10000):
    for _ in range(episodes):
        game = TicTacToe()
        state = game.board.copy()
        while True:
            action = agent.choose_action(state)
            reward = game.make_move(action[0], action[1])
            next_state = game.board.copy()

            # Update q-value
            agent.update_q_value(state, action, reward, next_state)

            state = next_state
            if reward is not None:
                print('game over.')  # game over
                break

#4. Play the Game
#Once the agent is trained, you can implement a game loop for users to play against the agent.

#Simplifications
"""- **Reward Structure**: The reward system here is simplistic. You can adjust rewards to promote better learning.
- **Epsilon Decay**: Consider implementing epsilon decay for exploration as training progresses.
- **Opponents**: Enhance complexity by allowing the agent to play against various opponents."""

#Conclusion
"""This approach is perfect as a starting point for creating a Tic Tac Toe bot using reinforcement learning with a
relatively simple structure, focusing on the Q-learning algorithm and a Q-table for state-action pair value storage.
It balances simplicity with the core concepts of reinforcement learning while being straightforward enough to build upon."""
